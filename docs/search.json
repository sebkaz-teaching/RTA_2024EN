[
  {
    "objectID": "lecture4.html",
    "href": "lecture4.html",
    "title": "Lecture 4 - Streaming architecture",
    "section": "",
    "text": "A streaming architecture is a defined set of technologies that work together to handle stream processing, which is the practice of taking action on a series of data at the time the data is created. In many modern deployments, Apache Kafka acts as the store for the streaming data, and then multiple stream processors can act on the data stored in Kafka to produce multiple outputs. Some streaming architectures include workflows for both stream processing and batch processing, which either entail other technologies to handle large-scale batch processing or use Kafka as the central store as specified in the Kappa Architecture.\nAn excellent real-time data processing architecture needs to be fault-tolerant and scalable; it needs to support batch and incremental updates and be extensible.\nAt the beginning, we explore two essential data processing architectures, Lambda and Kappa, that serve as the backbone of various enterprise applications.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4 - Streaming architecture"
    ]
  },
  {
    "objectID": "lecture4.html#lambda-architecture",
    "href": "lecture4.html#lambda-architecture",
    "title": "Lecture 4 - Streaming architecture",
    "section": "Lambda architecture",
    "text": "Lambda architecture\nLambda architecture comprises Batch Layer, Speed Layer (the Stream layer), and Serving Layer.\nThe batch layer operates on the complete data and thus allows the system to produce the most accurate results. However, the results come at the cost of high latency due to high computation time. The batch layer stores the raw data as it arrives and computes the batch views for consumption. Naturally, batch processes will occur at some interval and will be long-lived. The scope of data is from hours to years.\nThe speed layer generates results in a low-latency, near real-time fashion. The speed layer computes the real-time views to complement the batch views. The speed layer receives the arriving data and updates the batch layer results. The computation cost is significantly reduced thanks to the incremental algorithms implemented at the speed layer. The batch views may be processed with more complex or expensive rules and may have better data quality and less skew, while the real-time views give you up-to-the-moment access to the latest possible data.\nFinally, the serving layer enables various queries of the results sent from the batch and speed layers. The outputs from the batch layer in the form of batch views and the speed layer in the form of near-real-time opinions are forwarded to the serving layer, which uses this data to cater to the pending queries on an ad-hoc basis.\n \nImplementation Example:  \nGood\n\nGood balance of speed, reliability, and scalability.\nAccess to both real-time and offline results in covering many data analysis scenarios very well.\nAccessing a complete data set in a batch window may yield specific optimizations that make Lambda better performing and even simpler to implement.\n\nBad\n\nInternal processing logic is the same (batch and real-time layers) - many duplicate modules and coding.\nA data set modelled with Lambda architecture is difficult to migrate and reorganize.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4 - Streaming architecture"
    ]
  },
  {
    "objectID": "lecture4.html#kappa-architecture",
    "href": "lecture4.html#kappa-architecture",
    "title": "Lecture 4 - Streaming architecture",
    "section": "Kappa architecture",
    "text": "Kappa architecture\nThe Kappa Architecture is a software architecture used for processing streaming data. The central premise behind the Kappa Architecture is that you can perform real-time and batch processing, especially for analytics, with a single technology stack. It is based on a streaming architecture in which an incoming data series is first stored in a messaging engine like Apache Kafka. From there, a stream processing engine will read the data, transform it into an analyzable format, and then store it in an analytics database for end users to query.\nThe Kappa Architecture supports (near) real-time analytics when the data is read and transformed immediately after it is inserted into the messaging engine. This makes recent data quickly available for end-user queries. It also supports historical analytics by reading the stored streaming data from the messaging engine later in a batch manner to create additional analyzable outputs for more types of analysis.\nThe Kappa Architecture is a simpler alternative to the Lambda Architecture as it uses the same technology stack to handle real-time stream and historical batch processing. Both architectures entail the storage of historical data to enable large-scale analytics. Both architectures are also helpful in addressing “human fault tolerance,” in which problems with the processing code (bugs or known limitations) can be overcome by updating the code and rerunning it on historical data. The main difference with the Kappa Architecture is that all data is treated as a stream, so the stream processing engine acts as the sole data transformation engine.\n \nImplementation Example:  \n\nGood\n\nApplications can read and write directly to Kafka as developed. For existing event sources, listeners are used to streaming reports now from database logs eliminating the need for batch processing during ingress, resulting in fewer resources.\nQueries only need to look at a single serving location instead of going against batch and speed views.\n\n\n\nBad\n\nNot easy to implement, especially for the data replay.\n\n\n\nHow do the Lambda and Kappa compare?\nBoth architectures handle real-time and historical analytics in a single environment. However, one significant benefit of the Kappa Architecture over the Lambda Architecture is that it enables you to build your streaming and batch processing system on a single technology. This means you can build a stream processing application to handle real-time data, and if you need to modify your output, you update your code and then rerun it over the data in the messaging engine in a batch manner. As suggested by the Lambda Architecture, there is no separate technology to handle batch processing.\nWith a sufficiently fast-stream processing engine, you may not need a different technology optimized for batch processing. You read the stored streaming data in parallel (assuming the data in Kafka is appropriately split into separate channels, or “partitions”) and transform the data as if it were from a streaming source. For some environments, you can create the analyzable output on demand. When a new query is submitted from an end user, the data can be transformed ad hoc to answer that query optimally. Again, this requires a high-speed stream processing engine to enable low latency.\nWhile the Lambda Architecture does not specify the technologies that must be used, the batch processing component is often done on a large-scale data platform like Apache Hadoop. The Hadoop Distributed File System (HDFS) can economically store the raw data that can be transformed via Hadoop tools into an analyzable format. While Hadoop is used for the batch-processing component of the system, a separate engine designed for stream processing is used for the real-time analytics component. However, one advantage of the Lambda Architecture is that much larger data sets (in the petabyte range) can be stored and processed more efficiently in Hadoop for large-scale historical analysis.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4 - Streaming architecture"
    ]
  },
  {
    "objectID": "lecture4.html#publishsubscribe",
    "href": "lecture4.html#publishsubscribe",
    "title": "Lecture 4 - Streaming architecture",
    "section": "Publish/Subscribe",
    "text": "Publish/Subscribe\nThe Publish/Subscribe messaging system is critical to data-driven apps. Pub/Sub messaging is a pattern characterized by the sender (publisher) of a piece of data (message) not explicitly directing it to a receiver. Pub/sub-systems often have a broker, a central point where messages are.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4 - Streaming architecture"
    ]
  },
  {
    "objectID": "lecture4.html#apache-kafka",
    "href": "lecture4.html#apache-kafka",
    "title": "Lecture 4 - Streaming architecture",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nOn Kafka website, you’ll find the definition of it right on the first page:\n\nA distributed streaming platform\n\nWhat is an “Distributed streaming platform”?\nFirst, I want to remind you what a stream is. Streams are just unlimited data, data that never end. It just keeps arriving, and you can process it in real time.\nAnd distributed? Distributed means that Kafka works in a cluster, and each node in the group is called Broker. Those brokers are just servers executing a copy of apache Kafka.\nSo, Kafka is a set of machines working together to be able to handle and process real-time unlimited data.\nHis distributed architecture is one of the reasons that made Kafka so famous. The Brokers make it resilient, reliable, scalable, and fault-tolerant. That’s why Kafka is so performer and secure. But why is there this misconception that Kafka is another messaging system?\nTo respond to that answer, we must first explain how messaging works.\n\nMessaging system\nMessaging, very briefly, it’s just the act of sending a message from one place to another. It has three principal actors:\n\nProducer: Who produces and sends the messages to one or more queues;\nQueue: A buffer data structure that receives (from the producers) and delivers messages (to the consumers) in a FIFO (First-In First-Out) way. When a notice is given, it’s removed forever from the queue; there’s no chance of getting it back;\nConsumer: Who is subscribed to one or more queues and receives their messages when published.\n\nAnd that is it; this is how the messaging works (very briefly, there’s a lot more). As you can see, there’s nothing about streams, real-time, or clusters (depending on your chosen tool, you can use a set too, but it’s not native, like Kafka).\n\n\nKafka architecture\nA lot of information You can find in this link.\nNow that we know how messaging works let’s dive into the Kafka world. In Kafka, we also have the Producers and Consumers; they work in a very similar way that they work in the messaging producing and consuming messages.\n \nAs you can see, it is very similar to what we’ve discussed about messaging, but here we don’t have the Queue concept. Instead, we have the idea of Topics.\nThe Topic is a particular type of data stream; it’s very similar to a Queue, it receives and delivers messages as well, but there are some concepts that we need to understand about topics:\n\nA topic is divided into partitions; each case can have one or more partitions, and we need to specify that number when creating the issue. You can imagine the topic as a folder in the operating system and each folder inside her as a partition. \nEach message will be stored in the broker disk and receive an offset (unique identifier). This offset is unique at the partition level; each partition has its owns offsets. That is one more reason that makes Kafka so special, it stores the messages in the disk (like a database, and in fact, Kafka is a database too) to recover them later if necessary. Different from a messaging system, the message is deleted after being consumed;\nThe consumer use the offset to read the messages, from the oldest to the newest. In case of consumer failure, it will start reading from the final place when it recovers.  \n\n\n\nBrokers\nAs said before, Kafka works in a distributed way. A Kafka cluster may contain many brokers as needed.\n \nEach broker in a cluster is identified by an ID and contains at least one topic partition. To configure the number of partitions in each broker, we need to configure something called Replication Factor when creating a topic. Let’s say we have three brokers in our cluster, a topic with three partitions and a Replication Factor of three; in that case, each broker will be responsible for one section of the issue.\nAs you can see in the above image, \\(Topic_1\\) has three partitions; each broker is responsible for a section of the topic, so the Replication Factor of the \\(Topic_1\\) is three. The number of partitions must match the number of brokers; in this way, each broker will be responsible for a single section of the topic.\n\n\nProducers\nJust like in the messaging world, Producers in Kafka are the ones who produce and send the messages to the topics. As said before, the messages are sent in a round-robin way. Ex: Message 01 goes to partition 0 of Topic 1, and message 02 to partition 1 of the same topic. It means that we can’t guarantee that messages produced by the same producer will always be delivered to the same topic. We need to specify a key when sending the message, Kafka will generate a hash based on that key and will know what partition to deliver that message. That hash takes into consideration the number of the partitions of the topic, that’s why that number cannot be changed when the topic is already created.\n\n\n\nConsumers and Consumers Groups\nConsumers are applications subscribed to one or more topics that will read messages from there. They can read from one or more partitions. When a consumer reads from just one partition, we can ensure the order of the reading, but when a single consumer reads from two or more partitions, it will read in parallel, so, there’s no guarantee of the reading order. A message that came later can be read before another that came earlier, for example. That’s why we need to be careful when choosing the number of partitions and when producing the messages.\nAnother important concept of Kafka is the Consumer Groups. It’s really important when we need to scale the messages reading. It becomes very costly when a single consumer needs to read from many partitions, so, we need o load-balancing this charge between our consumers, this is when the consumer groups enter.\nThe data from a single topic will be load-balancing between the consumers, with that, we can guarantee that our consumers will be able to handle and process the data. The ideal is to have the same number of consumers in a group that we have as partitions in a topic, in this way, every consumer read from only one. When adding consumers to a group, you need to be careful, if the number of consumers is greater than the number of partitions, some consumers will not read from any topic and will stay idle.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 4 - Streaming architecture"
    ]
  },
  {
    "objectID": "lecture3.html",
    "href": "lecture3.html",
    "title": "Lecture 3 - Microservices architecture",
    "section": "",
    "text": "In the case of batch processing, we process historical data and the start time of the processing process has nothing to do with the time of occurrence of the analyzed events.\nFor streaming data, we have two time concepts:\n\nevent time - time in which the event happened.\nprocessing time - time during which the system processes the event.\n\nIn an ideal situation:  \nIn fact, data processing always takes place with a certain delay, which is represented by the points appearing below the function for the ideal situation (below the diagonal).\n \nIn stream processing applications, the differences between the time of the occurrence of an event and its processing prove to be important. The most common causes of delay are data transmission over the network or lack of communication between the device and the network. A simple example is driving a car through a tunnel and tracking the position via a GPS application.\nOf course, you can count the number of such missed events and trigger an alarm if there are too many such rejects.\nThe second (probably more often) used method is the use of the so-called correction watermarking.\nThe real-time event processing process can be represented as a step function, represented in the figure: \nAs can be seen, not all events contribute to the analysis and processing. The implementation of the processing process along with additional time for the occurrence of events (watermarking) can be presented as a process covering all events above the dashed line. The extra time allowed for additional events to be processed, but there may still be points that will not be taken into account. \nThe situations presented in the graphs clearly indicate why the concept of time is an important factor and requires precise definition already at the level of defining business needs. Timestamping data (events) is a difficult task.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 3 - Microservices architecture"
    ]
  },
  {
    "objectID": "lecture3.html#time-in-real-time-data-analysis",
    "href": "lecture3.html#time-in-real-time-data-analysis",
    "title": "Lecture 3 - Microservices architecture",
    "section": "",
    "text": "In the case of batch processing, we process historical data and the start time of the processing process has nothing to do with the time of occurrence of the analyzed events.\nFor streaming data, we have two time concepts:\n\nevent time - time in which the event happened.\nprocessing time - time during which the system processes the event.\n\nIn an ideal situation:  \nIn fact, data processing always takes place with a certain delay, which is represented by the points appearing below the function for the ideal situation (below the diagonal).\n \nIn stream processing applications, the differences between the time of the occurrence of an event and its processing prove to be important. The most common causes of delay are data transmission over the network or lack of communication between the device and the network. A simple example is driving a car through a tunnel and tracking the position via a GPS application.\nOf course, you can count the number of such missed events and trigger an alarm if there are too many such rejects.\nThe second (probably more often) used method is the use of the so-called correction watermarking.\nThe real-time event processing process can be represented as a step function, represented in the figure: \nAs can be seen, not all events contribute to the analysis and processing. The implementation of the processing process along with additional time for the occurrence of events (watermarking) can be presented as a process covering all events above the dashed line. The extra time allowed for additional events to be processed, but there may still be points that will not be taken into account. \nThe situations presented in the graphs clearly indicate why the concept of time is an important factor and requires precise definition already at the level of defining business needs. Timestamping data (events) is a difficult task.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 3 - Microservices architecture"
    ]
  },
  {
    "objectID": "lecture3.html#time-windows",
    "href": "lecture3.html#time-windows",
    "title": "Lecture 3 - Microservices architecture",
    "section": "time-windows",
    "text": "time-windows\nTumbling window is a fixed-length window. Its characteristic feature is that each event belongs to only one window.  \nSliding window includes all events occurring in a certain length among themselves.  \ndisjoint window has a fixed length, but allows one window to overlap another. Typically used to smooth data.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 3 - Microservices architecture"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Info",
    "section": "",
    "text": "Kod: 222891-D\nWinter semester 2022/2023, SGH Warsaw School of Economics\nBasics information about this course can be found in the syllabus.\nList of books! I recommend.\nIf You don’t know what Python is go here.",
    "crumbs": [
      "Home",
      "Info"
    ]
  },
  {
    "objectID": "index.html#real-time-analytics",
    "href": "index.html#real-time-analytics",
    "title": "Info",
    "section": "",
    "text": "Kod: 222891-D\nWinter semester 2022/2023, SGH Warsaw School of Economics\nBasics information about this course can be found in the syllabus.\nList of books! I recommend.\nIf You don’t know what Python is go here.",
    "crumbs": [
      "Home",
      "Info"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Info",
    "section": "Schedule",
    "text": "Schedule\n\nLectures\nLectures are OPTIONAL and takes place in G-208.\n\n\n26-02-2024 (Monday) 08:00-9:30 - Lecture 1\n\n\n04-03-2024 (Monday) 08:00-9:30 - Lecture 2\n\n\n11-03-2024 (Monday) 08:00-9:30 - Lecture 3\n\n\n18-03-2024 (Monday) 08:00-9:30 - Lecture 4\n\n\n25-03-2024 (Monday) 08:00-9:30 - Lecture 5\n\n\nLectures end with a TEST: 20 questions - 30 minutes.\nThe test will be conducted using MS Teams.\n\n\nLabs\n\n\n09-04-2024 (Tuesday) 9:50-11:30 - DS1-16\n\n\n16-04-2024 (Tuesday) 9:50-11:30 - DS1-16\n\n23-04-2024 (Tuesday) 9:50-11:30 - DS1-16\n30-04-2024 (Tuesday) 9:50-11:30 - DS1-16\n07-05-2024 (Tuesday) 9:50-11:30 - DS1-16\n14-05-2024 (Tuesday) 9:50-11:30 - DS1-16\n21-05-2024 (Tuesday) 9:50-11:30 - DS1-16\n28-05-2024 (Tuesday) 9:50-11:30 - DS1-16\n04-06-2023 (Tuesday) 9:50-11:30 - DS1-16\n\n\n\nPlace\nLectures 1-4: G-208 Labs 1-10: Sabinki 16\n\n\nExam\nThe lectures will conclude with a test on the last day of class. If you score above 13 points on the test, you will be eligible to carry out the exercises.\nOnce you have completed the exercises, you will need to submit your homework through the MS Teams platform. If you pass all the exercises and tasks, you will be allowed to complete the project.\nYou can work on the project in groups of up to five people. Here are the requirements for the project:\n\nYour project must address a BUSINESS PROBLEM that can be implemented using online information. You can use batch processing, such as generating a model, if needed.\nYou must send data to Apache Kafka for further processing and analysis.\nYou can use any programming language for your project components.\nBI tools are allowed.\nYour data sources can include tables, artificially generated data, IoT, and more.",
    "crumbs": [
      "Home",
      "Info"
    ]
  },
  {
    "objectID": "index.html#technology",
    "href": "index.html#technology",
    "title": "Info",
    "section": "Technology",
    "text": "Technology\nParticipating in the classes, you must know and at least use the following information technologies:\n\nGIT\nPython, Jupyter notebook\nDocker\nApache Spark, Apache Flink, Apache Kafka.",
    "crumbs": [
      "Home",
      "Info"
    ]
  },
  {
    "objectID": "lab/lab1.html",
    "href": "lab/lab1.html",
    "title": "Production Environment with ML Model",
    "section": "",
    "text": "Our task is to deploy a Python application implementing REST API tasks. Upon client request, the server will respond based on predictions generated from a certain model.\nThis image will be containerized using a Dockerfile, making server deployment possible regardless of the platform.\nWe will accomplish our task using the Flask library version 3.0.3.\nThere are other libraries that can accomplish this task.",
    "crumbs": [
      "Home",
      "Laboratories",
      "Production Environment with ML Model"
    ]
  },
  {
    "objectID": "lab/lab1.html#minimal-flask-application-code",
    "href": "lab/lab1.html#minimal-flask-application-code",
    "title": "Production Environment with ML Model",
    "section": "Minimal Flask Application Code",
    "text": "Minimal Flask Application Code\nWe want to run our application locally and then easily transfer and execute it on any computer. Therefore, a natural solution is to save the code to a file with the .py extension.\nTo automatically save the application code to the app.py file, we will use the magic command %%file file.py.\n\n%%file app.py\nfrom flask import Flask\n\n# Create a flask\napp = Flask(__name__)\n\n# Create an API end point\n@app.route('/')\ndef say_hello():\n    return \"Hello World\"\n\nif __name__ == '__main__':\n    app.run()\n\n\nNote! In the Flask documentation, the last two lines starting the server are not present in the sample code. Furthermore, the command to start the server is flask run, not python app.py. Let’s explain what the sample code contains:\n\n\nfrom flask import Flask: Imports the Flask library.\napp = Flask(__name__): Creates the API server interface.\nSubpage code using a decorator: Here, a route of the application is defined. Using a decorator (e.g., @app.route(‘/’)), you specify which URL address should trigger the associated function. Typically, the function returns the content that the client will receive when accessing this path.\n\n@app.route('/')\ndef say_hello():\n    return \"Hello World\"\nTo demonstrate how the decorator works, let’s define the following function:\n\ndef make_pretty(func):\n    def inner():\n        print(\"decorator working\")\n        func()\n    return inner()\n\n\ndef test():\n    print(\"abc\")\n\nmake_pretty(test)\n\ndecorator working\nabc\n\n\n\n@make_pretty\ndef test2():\n    print(\"test2\")\n\ndecorator working\ntest2\n\n\n\n@make_pretty\ndef test3():\n    print(\"anything else\")\n\ndecorator working\nanything else",
    "crumbs": [
      "Home",
      "Laboratories",
      "Production Environment with ML Model"
    ]
  },
  {
    "objectID": "lab/lab1.html#python-environment",
    "href": "lab/lab1.html#python-environment",
    "title": "Production Environment with ML Model",
    "section": "Python Environment",
    "text": "Python Environment\nIn order for the application code in app.py to run, we need a Python interpreter installed on our computer. However, simply having an interpreter is not sufficient for our application. To fully run it, we need to create an environment (preferably virtual) where all the necessary libraries (e.g., Flask) are available.\n\nNote: All terminal commands will be for Linux/Mac OS versions.\n\nwhich python\nwhich python3\nwhich pip \nwhich pip3\nAll these commands should point to the folder with the default Python environment.\nGenerate and run a virtual environment locally by entering the following command in the terminal:\npython3 -m venv .venv\nsource .venv/bin/activate\n\nGood practice: a Python environment is nothing more than a directory. In our version, it’s a hidden directory named .venv. If you copy this directory elsewhere, it will cease to function as your Python environment. Therefore, recreating it does not involve copying it. If your project is associated with a version control system like GIT, make sure the environment directory is not added to the repository. You can achieve this by adding the appropriate entry to the .gitignore file.\n\nHaving created a new environment, check which libraries are present in it.\npip list \n\nPackage    Version\n---------- -------\npip        23.2.1\npyspark    3.4.1\nsetuptools 65.5.0\nWe can check the Python and pip commands again\nwhich python\nwhich pip \nBy default, the libraries pip and setuptools should appear (pyspark comes from our internal image).\nInstall the Flask library:\npip install flask\npip list \nPackage      Version\n------------ -------\nblinker      1.7.0\nclick        8.1.7\nFlask        3.0.3\nitsdangerous 2.1.2\nJinja2       3.1.3\nMarkupSafe   2.1.5\npip          23.2.1\npyspark      3.4.1\nsetuptools   65.5.0\nWerkzeug     3.0.2\nAs you can see, installing the Flask library also forced the installation of other packages.\nThe only way to transfer the Python environment is to install it on a new machine and manually install all the packages. However, to avoid installing each package individually, we can use a configuration file called requirements.txt with a list of packages.\nRemember - each package should include the version number. Otherwise, it may turn out that new package versions break compatibility with your code. To create the configuration file, use the following command in the terminal:\npip freeze &gt;&gt; requirements.txt\nYou can use the generated file on any machine to install and recreate the required Python runtime environment.\n\nSide note: At the time of preparing the materials, Flask was in version 3.0.1 - today it is already available in version 3.0.3.\n\nTo install packages from the file, use the following command:\npip install -r requierements.txt\nWe now have two files: app.py and requirements.txt.\nBy moving them to any project on GitHub, we can run our application wherever a Python interpreter is available, allowing us to create a new virtual environment and install libraries from the requirements.txt file.\nFor full automation, it would be useful to be able to run the Python environment on any machine.\nTo achieve this, create a Dockerfile:\n\n%%file Dockerfile\nFROM python:3.11-slim-buster\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY app.py .\n\nENV FLASK_APP=app\n\nEXPOSE 8000\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\nThe above file allows Docker Desktop to run an image utilizing a basic operating system (here Linux) along with a basic Python 3.11 environment.\nFurthermore, this file copies necessary files (app.py, requirements.txt) into the Docker image.\nThe RUN command allows executing any bash command inside the Docker image.\nThe CMD command allows running a command to start the server in a mode that won’t close this command.\nThe last piece of information is setting the port to 8000.\n# creating docker container from Dockerfile\ndocker build -t modelML .\n# run container\ndocker run -p 8000:8000 modelML",
    "crumbs": [
      "Home",
      "Laboratories",
      "Production Environment with ML Model"
    ]
  },
  {
    "objectID": "lab/lab1.html#running-the-server-locally",
    "href": "lab/lab1.html#running-the-server-locally",
    "title": "Production Environment with ML Model",
    "section": "Running the Server Locally",
    "text": "Running the Server Locally\nThe server can be started in at least two ways.",
    "crumbs": [
      "Home",
      "Laboratories",
      "Production Environment with ML Model"
    ]
  },
  {
    "objectID": "lab/lab1.html#starting-the-server-via-terminal",
    "href": "lab/lab1.html#starting-the-server-via-terminal",
    "title": "Production Environment with ML Model",
    "section": "Starting the Server via Terminal",
    "text": "Starting the Server via Terminal\npython app.py\nor (if there is no app.run() code starting the server).\nflask run \nYou should see information similar to the following\n * Serving Flask app 'app'\n * Debug mode: off\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\n\nRunning the Server via Notebook\nDirectly running the code in a notebook will start the server and halt any further code execution. To avoid this, you can use the subprocess library.\n\nimport subporcess\n\np = subprocess.Popen([\"python\", \"app.py\"])\n\nIf we need to close the subprocess, execute\n\np.kill()\n\nWith the server running, you can query it using:\ncurl localhost:5000\nor in jupyter notebook:\n\nimport requests\n\nresponse = requests.get(\"http://127.0.0.1:5000/\")\n\nprint(response.content) # Hello World\nprint(response.status_code) # 200",
    "crumbs": [
      "Home",
      "Laboratories",
      "Production Environment with ML Model"
    ]
  },
  {
    "objectID": "lab/lab2.html",
    "href": "lab/lab2.html",
    "title": "Apache Kafka producer",
    "section": "",
    "text": "You can go through this version by having a new Docker image and Docker Desktop running on your own computer.\n\nOpen your web browser and navigate to the environment page (for Docker, open localhost:8888).\nLaunch a new terminal (in Jupyter Lab, using the terminal icon).\nNavigate to the main directory and list all elements. Check if the kafka directory is on the list.\n\n   cd ~\n   ls -all\n\nRun the command to check the list of Kafka server topics.\n\n    kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n\nAdd a topic named “streaming”\n\n   kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streaming\n\nCheck the list of topics again, making sure you have the “streaming” topic.\nOpen a new terminal in the notebook and create a producer in the console generating data for the new topic.\n\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic streaming\nTo check if message sending works, open another terminal window and enter the following command to run a consumer in the console:\nkafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming --from-beginning\n\nRemember to run commands from the appropriate directory.\n\n\n%%file stream.py\n\nimport json\nimport random\nimport sys\nfrom datetime import datetime, timedelta\nfrom time import sleep\n\nfrom kafka import KafkaProducer\n\nif __name__ == \"__main__\":\n    SERVER = \"broker:9092\"\n\n    producer = KafkaProducer(\n        bootstrap_servers=[SERVER],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n        api_version=(3, 7, 0),\n    )\n    \n    try:\n        while True:\n            \n            t = datetime.now() + timedelta(seconds=random.randint(-15, 0))\n            \n            message = {\n                \"time\" : str(t),\n                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"]),\n                \"values\" : random.randint(0,100)\n            }\n            \n            \n            producer.send(\"streaming\", value=message)\n            sleep(1)\n    except KeyboardInterrupt:\n        producer.close()",
    "crumbs": [
      "Home",
      "Laboratories",
      "Apache Kafka producer"
    ]
  },
  {
    "objectID": "lab/lab2.html#apache-kafka-with-our-env",
    "href": "lab/lab2.html#apache-kafka-with-our-env",
    "title": "Apache Kafka producer",
    "section": "",
    "text": "You can go through this version by having a new Docker image and Docker Desktop running on your own computer.\n\nOpen your web browser and navigate to the environment page (for Docker, open localhost:8888).\nLaunch a new terminal (in Jupyter Lab, using the terminal icon).\nNavigate to the main directory and list all elements. Check if the kafka directory is on the list.\n\n   cd ~\n   ls -all\n\nRun the command to check the list of Kafka server topics.\n\n    kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n\nAdd a topic named “streaming”\n\n   kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streaming\n\nCheck the list of topics again, making sure you have the “streaming” topic.\nOpen a new terminal in the notebook and create a producer in the console generating data for the new topic.\n\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic streaming\nTo check if message sending works, open another terminal window and enter the following command to run a consumer in the console:\nkafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming --from-beginning\n\nRemember to run commands from the appropriate directory.\n\n\n%%file stream.py\n\nimport json\nimport random\nimport sys\nfrom datetime import datetime, timedelta\nfrom time import sleep\n\nfrom kafka import KafkaProducer\n\nif __name__ == \"__main__\":\n    SERVER = \"broker:9092\"\n\n    producer = KafkaProducer(\n        bootstrap_servers=[SERVER],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n        api_version=(3, 7, 0),\n    )\n    \n    try:\n        while True:\n            \n            t = datetime.now() + timedelta(seconds=random.randint(-15, 0))\n            \n            message = {\n                \"time\" : str(t),\n                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"]),\n                \"values\" : random.randint(0,100)\n            }\n            \n            \n            producer.send(\"streaming\", value=message)\n            sleep(1)\n    except KeyboardInterrupt:\n        producer.close()",
    "crumbs": [
      "Home",
      "Laboratories",
      "Apache Kafka producer"
    ]
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O’REILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzyśko, Wołyński, Górecki, Skorzybut, Systemy uczące się . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Prószyński i s-ka"
  },
  {
    "objectID": "books.html#books",
    "href": "books.html#books",
    "title": "Books and WWW pages",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\n\n\n\n\n\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup\nG. Shapira, T. Palino, R. Sivaram, K. Petty Kafka The Definitive Guide. Real-time data and stream processing at scale. Second edition, 2021. O’REILLY.\nJ. Korstanje Machine Learning for Streaming Data with Python, 2022. PACKT\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nSebastian Raschka Python. Uczenie maszynowe. Wydanie II. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska IPython. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\nT. Morzy Eksploracja Danych. Metody i algorytmy, PWN, 2013.\nKrzyśko, Wołyński, Górecki, Skorzybut, Systemy uczące się . WNT, 2008\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nSpark. Zaawansowana analiza danych (ebook) Zobacz opis lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\nJ. Quddus Machine Learning with Apache Spark Quick Start Guide (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuka tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nL. Suskind, Mechanika kwantowa, teoretyczne minimum, 2014, Prószyński i s-ka"
  },
  {
    "objectID": "books.html#www-pages",
    "href": "books.html#www-pages",
    "title": "Books and WWW pages",
    "section": "WWW Pages",
    "text": "WWW Pages\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPython libraries for data analysis\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nText editors\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nETL\n\ndata cookbook\n\n\n\nDatasets\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nML course\n\nKurs Machine Learning - Andrew Ng, Stanford"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Real Time Analytics\nSGH Warsaw School of Economics\nECTS: 3\nLanguage: EN\nlevel: medium\nday of week: Monday/Tuesday\nTeacher: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: http://sebkaz-teaching.github.io/EN"
  },
  {
    "objectID": "sylabus.html#description",
    "href": "sylabus.html#description",
    "title": "Syllabus",
    "section": "Description",
    "text": "Description\nMaking the right decisions based on data and their analysis in business is a process and daily. Modern methods of modeling by machine learning (ML), artificial intelligence (AI), or deep learning not only allow better understanding of business, but also support making key decisions for it. The development of technology and increasingly new business concepts of working directly with the client require not only correct but also fast decisions. The classes offered are designed to provide students with experience and comprehensive theoretical knowledge in the field of real-time data processing and analysis, and to present the latest technologies (free and commercial) for the processing of structured data (originating e.g. from data warehouses) and unstructured (e.g. images, sound, video streaming) in on-line mode. The course will present the so called lambda and kappa structures for data processing into data lake along with a discussion of the problems and difficulties encountered in implementing real-time modeling for large amounts of data. Theoretical knowledge will be gained (apart from the lecture part) through the implementation of test cases in tools such as Apache Spark, Nifi, Microsoft Azure and SAS. During laboratory classes student will benefit from fully understand the latest information technologies related to real-time data processing."
  },
  {
    "objectID": "sylabus.html#list-of-topics",
    "href": "sylabus.html#list-of-topics",
    "title": "Syllabus",
    "section": "List of Topics",
    "text": "List of Topics\n\nModelling, learning and prediction in batch mode (offline learning) and incremental (online learning) modes. Problems of incremental machine learning.\nData processing models in Big Data. From flat files to Data Lake. Real-time data myth and facts\nNRT systems (near real-time systems), data acquisition, streaming and analytics.\nAlgorithms for estimating model parameters in incremental mode. Stochastic Gradient Descent.\nLambda and Kappa architecture. Designing IT architecture for real-time data processing.\nPreparation of the micro-service with the ML model for prediction use.\nStructured and unstructured data. Relational databases and NoSQL databases.\nAggregations and reporting in NoSQL databases (on the example of the MongoDB or Cassandra)\nBasic of object-oriented programming in Python in linear and logistic regression, neural network analysis using the sklearn, TensorFlow and Keras.\nIT architecture of Big Data processing. Preparation of a virtual env for Apache Spark."
  },
  {
    "objectID": "sylabus.html#conditions-for-passing",
    "href": "sylabus.html#conditions-for-passing",
    "title": "Syllabus",
    "section": "Conditions for passing",
    "text": "Conditions for passing\n\ntest 30%\npractical test 30% (IF)\ngroup project 40% (70%)"
  },
  {
    "objectID": "sylabus.html#books",
    "href": "sylabus.html#books",
    "title": "Syllabus",
    "section": "Books",
    "text": "Books\n\nS. Zajac, “Modelowanie dla biznesu, Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe”. SGH (2022)\nFrątczak E., red. “Modelowanie dla biznesu, Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring”. SGH, Warszawa 2019.\nFrątczak E., red., “Zaawansowane metody analiz statystycznych”, Oficyna Wydawnicza SGH, Warszawa 2012.\nIndest A., Wild Knowledge. Outthik the Revolution. LID publishing.com 2017.\nReal Time Analytic. “The Key to Unlocking Customer Insights & Driving the Customer Experience”. Harvard Business Review Analytics Series, Harvard Business School Publishing, 2018.\nSvolba G., “Applying Data Science. Business Case Studies Using SAS”. SAS Institute Inc., Cary NC, USA, 2017.\nEllis B. “Real-Time Analytics Techniques to Analyze and Visualize Streaming data.” , Wiley, 2014\nFamiliar B., Barnes J. “Business in Real-Time Using Azure IoT and Cortana Intelligence Suite” Apress, 2017"
  },
  {
    "objectID": "lab/lab3.html",
    "href": "lab/lab3.html",
    "title": "Structured data",
    "section": "",
    "text": "# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Home",
      "Laboratories",
      "Structured data"
    ]
  },
  {
    "objectID": "lab/lab3.html#data-as-variables",
    "href": "lab/lab3.html#data-as-variables",
    "title": "Structured data",
    "section": "Data as variables",
    "text": "Data as variables\n\n# variables\ncustomer1_age = 38\ncustomer1_height = 178\ncustomer1_loan = 34.23\ncustomer1_name = 'Zajac'\n\n\nWhy don’t we use variables for data analysis?\n\nIn Python, regardless of the type of data being analyzed and processed, we can collect data and represent it as a form of list.\n\n# python lists - what we can put on list ?\ncustomer = []\nprint(customer)\n\n\n# different types in one object\ntype(customer)\n\n\nWhy lists aren’t the best place to store data?\n\nLet’s take two numerical lists.”\n\n# two numerical lists\na = [1,2,3]\nb = [4,5,6]\n\nTypical operations on lists in data analysis\n\n# add lists\nprint(f\"a+b: {a+b}\")\n# we can use .format also \nprint(\"a+b: {}\".format(a+b))\n\n\n# multiplication\ntry:\n    print(a*b)\nexcept TypeError:\n    print(\"no-defined operation\")\n\n\nimport numpy as np\naa = np.array(a)\nbb = np.array(b)\n\nprint(aa,bb)\n\n\nprint(f\"aa+bb: {aa+bb}\")\n# add - working\ntry:\n    print(\"=\"*50)\n    print(aa*bb)\n    print(\"aa*bb - is this correct ?\")\n    print(np.dot(aa,bb))\n    print(\"np.dot - is this correct ?\")\nexcept TypeError:\n    print(\"no-defined operation\")\n# multiplication\n\n\n# array properties\nx = np.array(range(4))\nprint(x)\nx.shape\n\n\nA = np.array([range(4),range(4)])\n# transposition  row i -&gt; column j, column j -&gt; row i \nA.T\n\n\n# 0-dim object\nscalar = np.array(5)\nprint(f\"scalar object dim: {scalar.ndim}\")\n# 1-dim object\nvector_1d = np.array([3, 5, 7])\nprint(f\"vector object dim: {vector_1d.ndim}\")\n# 2 rows for 3 features\nmatrix_2d = np.array([[1,2,3],[3,4,5]])\nprint(f\"matrix object dim: {matrix_2d.ndim}\")\n\n\nSebastian Raschka Course",
    "crumbs": [
      "Home",
      "Laboratories",
      "Structured data"
    ]
  },
  {
    "objectID": "lab/lab3.html#pytorch",
    "href": "lab/lab3.html#pytorch",
    "title": "Structured data",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch is an open-source Python-based deep learning library. PyTorch has been the most widely used deep learning library for research since 2019 by a wide margin. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features.\n\nPyTorch is a tensor library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\nPyTorch is an automatic differentiation engine, also known as autograd, which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.\nPyTorch is a deep learning library, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers.\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\n\ntensor0d = torch.tensor(1) \ntensor1d = torch.tensor([1, 2, 3])\ntensor2d = torch.tensor([[1, 2, 2], [3, 4, 5]])\ntensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n\nprint(tensor1d.dtype)\n\n\ntorch.tensor([1.0, 2.0, 3.0]).dtype\n\n\ntensor2d\n\n\ntensor2d.shape\n\n\nprint(tensor2d.reshape(3, 2))\n\n\nprint(tensor2d.T)\n\n\nprint(tensor2d.matmul(tensor2d.T))\n\n\nprint(tensor2d @ tensor2d.T)\n\nmore info on pytorch",
    "crumbs": [
      "Home",
      "Laboratories",
      "Structured data"
    ]
  },
  {
    "objectID": "lab/lab3.html#data-modeling",
    "href": "lab/lab3.html#data-modeling",
    "title": "Structured data",
    "section": "Data Modeling",
    "text": "Data Modeling\nLet’s take one variable (xs) and one target variable (ys - target).\nxs = np.array([-1,0,1,2,3,4])\nys = np.array([-3,-1,1,3,5,7])\nWhat kind of model we can use?\n\n# Regresja liniowa \n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nxs = np.array([-1,0,1,2,3,4])\n# a raczej \nxs = xs.reshape(-1, 1)\n\nys = np.array([-3, -1, 1, 3, 5, 7])\n\nreg = LinearRegression()\nmodel = reg.fit(xs,ys)\n\nprint(f\"solution: x1={model.coef_[0]}, x0={reg.intercept_}\")\n\nmodel.predict(np.array([[1],[5]]))\n\nThe simple code fully accomplishes our task of finding a linear regression model.\nWhat can we use such a generated model for?\nTo make use of it, we need to export it to a file.\n\n# save model\nimport pickle\nwith open('model.pkl', \"wb\") as picklefile:\n    pickle.dump(model, picklefile)\n\nNow we can import it (for example, on GitHub) and utilize it in other projects.\n\n# load model\nwith open('model.pkl',\"rb\") as picklefile:\n    mreg = pickle.load(picklefile)\n\nBut !!! remember about Python Env\n\nmreg.predict(xs)",
    "crumbs": [
      "Home",
      "Laboratories",
      "Structured data"
    ]
  },
  {
    "objectID": "lab/lab3.html#neural-networks",
    "href": "lab/lab3.html#neural-networks",
    "title": "Structured data",
    "section": "Neural Networks",
    "text": "Neural Networks\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\n\nimport tensorflow as tf\n\nWe can also look at this problem from a different perspective. Neural networks are also capable of solving regression problems\n\nlayer_0 = Dense(units=1, input_shape=[1])\n\nmodel = Sequential([layer_0])\n\n# compiling and fits\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\nmodel.fit(xs, ys, epochs=10)\n\n\nprint(f\"{layer_0.get_weights()}\")\n\nOther ways of acquiring data\n\nReady-made sources in Python libraries.\nData from external files (e.g., CSV, JSON, TXT) from a local disk or the internet.\nData from databases (e.g., MySQL, PostgreSQL, MongoDB).\nData generated artificially for a chosen modeling problem.\nData streams.\n\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\n\n# find all keys\niris.keys()\n\n\n# print description\nprint(iris.DESCR)\n\n\nimport pandas as pd\nimport numpy as np\n\n# create DataFrame\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                  columns= iris['feature_names'] + ['target'])\n\n\n# show last\ndf.tail(10)\n\n\n# show info about NaN values and a type of each column.\ndf.info()\n\n\n# statistics\ndf.describe()\n\n\n# new features\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n\n# remove features (columns) \ndf = df.drop(columns=['target'])\n# filtering first 100 rows and 4'th column\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\", palette=\"husl\")\n\niris_melt = pd.melt(df, \"species\", var_name=\"measurement\")\nf, ax = plt.subplots(1, figsize=(15,9))\nsns.stripplot(x=\"measurement\", y=\"value\", hue=\"species\", data=iris_melt, jitter=True, edgecolor=\"white\", ax=ax)\n\n\nX = df.iloc[:100,[0,2]].values\ny = df.iloc[0:100,4].values\n\n\ny = np.where(y == 'setosa',-1,1)\n\n\nplt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa')\nplt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\nFor this type of linearly separable data, use logistic regression model or neural network.\n\nfrom sklearn.linear_model import Perceptron\n\nper_clf = Perceptron()\nper_clf.fit(X,y)\n\ny_pred = per_clf.predict([[2, 0.5],[4,5.5]])\ny_pred",
    "crumbs": [
      "Home",
      "Laboratories",
      "Structured data"
    ]
  },
  {
    "objectID": "lab/lab3.html#data-storage-and-connection-to-a-simple-sql-database",
    "href": "lab/lab3.html#data-storage-and-connection-to-a-simple-sql-database",
    "title": "Structured data",
    "section": "Data Storage and Connection to a Simple SQL Database",
    "text": "Data Storage and Connection to a Simple SQL Database\n\nIRIS_PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\ncol_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\ndf = pd.read_csv(IRIS_PATH, names=col_names)\n\n\n# save to sqlite\nimport sqlite3\n# generate database\nconn = sqlite3.connect(\"iris.db\")\n# pandas to_sql\n\ntry:\n    df.to_sql(\"iris\", conn, index=False)\nexcept:\n    print(\"tabela już istnieje\")\n\n\n# sql to pandas\nresult = pd.read_sql(\"SELECT * FROM iris WHERE sepal_length &gt; 5\", conn)\n\n\nresult.head(3)\n\n\n# Artificial data\nfrom sklearn import datasets\nX, y = datasets.make_classification(n_samples=10**4,\nn_features=20, n_informative=2, n_redundant=2)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# train test split by heand\ntrain_samples = 7000 # 70% \n\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\n\nrfc.predict(X_train[0].reshape(1, -1))",
    "crumbs": [
      "Home",
      "Laboratories",
      "Structured data"
    ]
  },
  {
    "objectID": "lab/lab3.html#zadania",
    "href": "lab/lab3.html#zadania",
    "title": "Structured data",
    "section": "ZADANIA",
    "text": "ZADANIA\n\nLoad data from the train.csv file and put it into the panda’s data frame\n\n\n## YOUR CODE HERE\ndf = \n\n\nShow number of row and number of columns\n\n\n## YOUR CODE HERE\n\nPerform missing data handling:\n\nOption 1 - remove rows containing missing data (dropna())\nOption 2 - remove columns containing missing data (drop())\nOption 3 - perform imputation using mean values (fillna())\n\nWhich columns did you choose for each option and why?\n\n## YOUR CODE HERE\n\n\nUsing the nunique() method, remove columns that are not suitable for modeling.\n\n\n## YOUR CODE HERE\n\n\nConvert categorical variables using LabelEncoder into numerical form.\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n## YOUR CODE HERE\n\n\nUtilize MinMaxScaler to transform floating-point data to a common scale\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n## YOUR CODE HERE\n\n\nSplit the data into training set (80%) and test set (20%)\n\n\nfrom sklearn.model_selection import train_test_split\n## YOUR CODE HERE\nX_train, X_test, y_train, y_test = train_test_split(...., random_state=44)\n\n\nUsing mapping, you can classify each passenger. The run() function requires providing a classifier for a single case.\n\nWrite a classifier that assigns a value of 0 or 1 randomly (you can use the random.randint(0,1) function).\nExecute the evaluate() function and check how well the random classifier performs.”\n\n\n\nclassify = ...\n\n\ndef run(f_classify, x):\n    return list(map(f_classify, x))\n\ndef evaluate(predictions, actual):\n    correct = list(filter(\n        lambda item: item[0] == item[1],\n        list(zip(predictions, actual))\n    ))\n    return f\"{len(correct)} correct answers from {len(actual)}. Accuracy ({len(correct)/len(actual)*100:.0f}%)\"\n\n\nevaluate(run(classify, X_train.values), y_train.values)",
    "crumbs": [
      "Home",
      "Laboratories",
      "Structured data"
    ]
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn’t use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#python-env-with-jupyter-lab",
    "href": "info.html#python-env-with-jupyter-lab",
    "title": "All tools",
    "section": "",
    "text": "For our first a few laboratories we will use just python codes. Check what is Your Python3 environment.\nIn the terminal try first:\npython\n# and\npython3\nI have python3 (You shouldn’t use python 2.7 version) so i create a new and a clear python environment.\nThe easiest way how to run a JupyterLab with your new python env. For  You can choose what You want.\npython3 -m venv &lt;name of Your env&gt;\n\nsource &lt;name of your env&gt;/bin/activate\n# . env/bin/activate\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# or\npip install -r requirements.txt\n\njupyterlab\ngo to web browser: localhost:8888\nIf You want rerun jupyterlab (after computer reset) just go to Your folder and run:\nsource &lt;name of your env&gt;/bin/activate\njupyterlab"
  },
  {
    "objectID": "info.html#start-working-with-github",
    "href": "info.html#start-working-with-github",
    "title": "All tools",
    "section": "Start working with GitHub",
    "text": "Start working with GitHub\nText from web site\nWhen You working on a project, e.g. a master’s thesis, (alone or in a team) you often need to check what changes, when and by whom were introduced to the project. The “version control system” or GIT works great for this task.\nYou can download and install Git like a regular program on any computer. However, most often (small projects) you use websites with some kind of git system. One of the most recognized is GitHub (www.github.com) which allows you to use the git system without installing it on your computer.\nIn the free version of the GitHub website, you can store your files in public (everyone has access) repositories. We will only focus on the free version of GitHub:\ngit --version"
  },
  {
    "objectID": "info.html#github",
    "href": "info.html#github",
    "title": "All tools",
    "section": "GitHub",
    "text": "GitHub\nAt the highest level, there are individual accounts (eg. http://github.com/sebkaz or those set up by organizations. Individual users can create ** repositories ** public (public) or private (private).\nOne file should not exceed 100 MB.\nRepo (shortcut to repository) is created with Create a new repository. Each repo should have an individual name.\n\nBranches\nThe main (created by default) branch of the repository is named master.\n\n\nMost important commends\n\nclone of Your repository\n\ngit clone https://adres_repo.git\n\nIn github case, you can download the repository as a ‘zip’ file.\n\n\nRepository for local directory\n\n# new directory\nmkdir datamining\ncd datamining\n# init repo\ngit init\n# there sould be a .git new directory\n# add file\necho \"Info \" &gt;&gt; README.md\n\nlocal and web version connection\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\n3 steps\n\n# status check\ngit status\n# 1. add all changes\ngit add .\n# 2. commit all changes with message\ngit commit -m \" message \"\n# 3. and\ngit push origin master\nYou can watch Youtube course.\nAll the necessary programs will be delivered in the form of docker containers."
  },
  {
    "objectID": "info.html#start-with-docker",
    "href": "info.html#start-with-docker",
    "title": "All tools",
    "section": "Start with Docker",
    "text": "Start with Docker\nIn order to download the docer software to your system, go to the page.\nIf everything is installed correctly, follow these instructions:\n\nCheck the installed version\n\ndocker --version\n\nDownload and run the image Hello World and\n\ndocker run hello-world\n\nOverview of downloaded images:\n\ndocker image ls\n\ndocker images\n\nOverview of running containers:\n\ndocker ps \n\ndocker ps -all\n\nStopping a running container:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nContainer removal\n\ndocker rm -f &lt;CONTAINER ID&gt;\nI also recommend short intro"
  },
  {
    "objectID": "lecture1.html",
    "href": "lecture1.html",
    "title": "From files to Data Mash",
    "section": "",
    "text": "In recent times, a new way of handling data called stream processing has become important for companies big and small.\nIt’s like a supercharged version of how we used to work with data.\nThis new method is really good at solving the problems that old-fashioned data systems had trouble with, especially things like:\nStream processing offers several advantages over traditional batch processing methodologies:\nThis lecture aims to explain the growing excitement around stream processing and its transformative impact on data-driven application development.\nBefore we dive into stream processing, let’s first understand traditional data application architectures and their limitations.\nIn the past, data applications have mostly used batch processing. This means data is processed in separate chunks at specific times. Think of it like baking cookies - you can only bake as many as fit on the tray at one time.\nBatch processing works well in some cases, but it has problems when it comes to real-time data processing.\nImagine you’re trying to track the score of a live football game. If you’re using batch processing, you might only get updates at half-time and at the end of the game. This is because batch processing can cause delays, known as latency issues.\nBatch processing also has trouble scaling, which means it can’t always handle large amounts of data efficiently. It’s like trying to bake cookies for a whole city with just one oven!\nMoreover, batch processing is not very flexible. If the data changes or something unexpected happens, batch processing can’t adapt quickly. This can slow down businesses and prevent them from innovating.\nIn the next part of the lecture, we’ll discuss how stream processing can help overcome these limitations.",
    "crumbs": [
      "Home",
      "Lectures",
      "From files to Data Mash"
    ]
  },
  {
    "objectID": "lecture1.html#data",
    "href": "lecture1.html#data",
    "title": "From files to Data Mash",
    "section": "Data",
    "text": "Data\nWhen you think of data, you may think of piles of bytes sitting in data warehouses, in rational databases, or on distributed filesystems. Systems like these have trained us to think of data in its resting state. When you need to process it, you run some query or job against the pile of bytes.\nThe advancement of information technology has revolutionized the way we access and utilize data. With the availability of vast amounts of both structured and unstructured data, new opportunities have emerged for scientific and business challenges.\nData plays a crucial role in the creation, collection, storage, and processing of information on an unprecedented scale. This has led to the development of numerous tools and technologies that enable us to harness the power of data.\nThanks to open-source software and the computing power of home computers, we can now tackle complex problems and explore new frontiers in various domains. The possibilities are endless as we continue to push the boundaries of data-driven innovation.\nThe new era of business and scientific challenges brings forth a multitude of opportunities:\n\nIntelligent advertising for thousands of products, targeting millions of customers.\nProcessing of data related to genes, RNA, or proteins, such as genus.\nIntelligent detection of fraudulent activities among hundreds of billions of credit card transactions.\nStock market simulations based on thousands of financial instruments.\n…\n\nAs we enter the data age, we face not only the challenge of handling large quantities of data but also the need for faster data processing.\nMachine learning algorithms rely on structured data in tabular form. This data is organized into columns representing characteristics that describe each observation or row. For example, these characteristics could include sex, height, or the number of cars owned. These features are used to predict whether a customer will repay a loan or not. This prediction is also included as a feature. By utilizing tables of features created in this manner, we can employ algorithms like XGBoost or logistic regression to determine the optimal combination of variables that influence the probability of a good or bad customer.\nUnstructured data refers to data that does not have a predefined structure or format, such as sound, images, and text. Unlike structured data, which is organized in a tabular form with columns and rows, unstructured data lacks a consistent organization.\nWhen processing unstructured data, it is often converted into a vector form to enable analysis and extraction of meaningful insights. However, individual elements like letters, frequencies, or pixels do not convey specific information on their own. They need to be transformed and analyzed collectively to derive valuable features and patterns.\nUnderstanding the distinction between structured and unstructured data is crucial for effective data processing and analysis.\n\nGive an example of structured and unstructured data. Load sample data in jupyter notebook.\n\n\nKnows the types of structured and unstructured data (K2A_W02, K2A_W04, O2_W04, O2_W07)",
    "crumbs": [
      "Home",
      "Lectures",
      "From files to Data Mash"
    ]
  },
  {
    "objectID": "lecture1.html#data-sources",
    "href": "lecture1.html#data-sources",
    "title": "From files to Data Mash",
    "section": "Data sources",
    "text": "Data sources\nThe three largest data generators are:\n\nSocial data: This includes texts (tweets, social network posts, comments), photos, and videos. Social data is valuable for consumer behavior analysis and sentiment analysis in marketing.\nSensor and log data: These are generated by various devices and users, such as IoT devices and website logs. Sensor and log data play a crucial role in IoT technology and are widely used in data processing and business applications.\nTransaction data: This type of data is generated from online and offline transactions. Transaction data is essential for performing transactions and conducting comprehensive analytics across various domains.",
    "crumbs": [
      "Home",
      "Lectures",
      "From files to Data Mash"
    ]
  },
  {
    "objectID": "lecture1.html#actual-data-generation-process",
    "href": "lecture1.html#actual-data-generation-process",
    "title": "From files to Data Mash",
    "section": "Actual data generation process",
    "text": "Actual data generation process\nThe data generated in reality is a continuous stream produced by various systems and devices. For example, your phone alone generates a significant amount of data every day. This continuous generation of data is not limited to a specific time frame.\nBatch processing, which involves dividing data into chunks based on a specified time interval, is one approach to processing data. However, this approach may not always be suitable, especially when dealing with real-time data streams.\nTo handle the continuous flow of data from different sources, various systems have been designed, including data warehouses, IoT device monitoring systems, transaction systems, website analytics systems, internet advertising platforms, social media platforms, and operating systems.\nThese systems play a crucial role in processing and analyzing the data stream, enabling organizations to gain valuable insights and make informed decisions.\n\nA company is an organization that works and responds to a constant stream of data.\n\nIn traditional data processing, the input to the data source (and also the output of the evaluation) is typically a file. This file is written once and can be referenced by multiple functions or tasks.\nOn the other hand, in stream processing, data flows through a continuous stream of changes. This stream is generated by a source, often referred to as the “manufacturer”, “sender”, or “supplier”. The stream can be consumed by multiple recipients, known as “consumers”. The events in the stream are organized into topics.\nTopics serve as a way to group related events together, allowing for efficient processing and analysis of the streaming data.",
    "crumbs": [
      "Home",
      "Lectures",
      "From files to Data Mash"
    ]
  },
  {
    "objectID": "lecture1.html#not-to-big-data",
    "href": "lecture1.html#not-to-big-data",
    "title": "From files to Data Mash",
    "section": "not to Big Data",
    "text": "not to Big Data\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.’’ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, … four V\n\nVolume - The amount of data being generated worldwide is increasing rapidly. Students can benefit from understanding how to handle and analyze large volumes of data efficiently.\nVelocity - Data is being produced and transferred at a high speed. Students need to learn how to process and analyze data in real-time to keep up with the fast-paced nature of data streams.\nVariety - Data comes in various forms, including text, images, videos, and IoT data. Students should be familiar with different data types and know how to work with them effectively.\nVeracity - Students need to understand the importance of data quality and reliability. They should learn how to evaluate the accuracy and completeness of data to make informed decisions based on reliable information.\nValue - Students should recognize the value that data holds and how it can be used to gain insights and make informed decisions. Understanding the potential benefits and costs associated with data analysis is crucial for students in various fields of study.\n\n\nThe purpose of calculations is not numbers, but understanding them R.W. Hamming 1962.\n\nData and data processing have been integral to businesses for many decades. As time has passed, the collection and utilization of data have steadily increased, leading companies to develop robust infrastructures to manage and process this data effectively.",
    "crumbs": [
      "Home",
      "Lectures",
      "From files to Data Mash"
    ]
  },
  {
    "objectID": "lecture1.html#data-processing-models",
    "href": "lecture1.html#data-processing-models",
    "title": "From files to Data Mash",
    "section": "Data processing models",
    "text": "Data processing models\n\nTraditional Data Processing Model\nIn the traditional data processing model, businesses typically implement two types of data processing:\n\nOnline Transaction Processing (OLTP): This model is suitable for ongoing services such as customer service, order registration, or sales. It involves applications that interact with external services or human users and continuously process incoming events. When an event is processed, the application reads or updates its state by running transactions against a remote database system. This model provides effective solutions for data storage, transactional data recovery, data access optimization, concurrency management, and event processing.\nDatabase Query Model: Most of the data is stored in databases or data warehouses, and access to the data is implemented through queries via applications. This model focuses on the method of using and implementing the database access process. The two most commonly used implementations are:\n\nRelational Database Model: This model organizes data into tables with predefined relationships between them. It uses Structured Query Language (SQL) to perform operations such as querying, inserting, updating, and deleting data.\nData Warehouse Model: This model is designed for analytical processing and decision-making. It involves extracting, transforming, and loading data from various sources into a central repository, known as a data warehouse. The data in the warehouse is organized in a way that facilitates complex queries and analysis.\n\n\nUnderstanding these traditional data processing models is essential for businesses to effectively manage and process their data.\n\n\nThis model offers effective solutions for:\n\nEfficient and secure data storage\nTransactional data recovery in case of failures\nOptimization of data access\nConcurrency management\nEvent processing, involving reading and writing data\n\nIn addition to the benefits, the new data processing model and data warehouse database are particularly useful in scenarios involving:\n\nAggregation of data from multiple systems, such as consolidating data from various stores or branches.\nSupporting data analysis by providing a centralized repository for performing complex queries and analysis.\nGenerating data reports and summaries for business intelligence purposes.\nOptimizing complex queries to improve performance and efficiency.\nSupporting informed business decisions by providing reliable and up-to-date data.\n\nThese capabilities make the new data processing model and data warehouse database an ideal choice for organizations that require robust data management and analysis capabilities.\nResearch on such issues has led to the formulation of a new data processing model and a new type of database (Data warehouse).\nThis application design can cause problems when applications need to evolve or scale. Since multiple applications might work on the same data representation or share the same infrastructure, changing the schema of a table or scaling a database system requires careful planning and a lot of effort. Currently, many running applications (even in one area) are implemented as microservices, i.e. small and independent applications (LINUX programming philosophy - do little but right). Because microservices are strictly decoupled from each other and only communicate over well-defined interfaces, each microservice can be implemented with a different technology stack including a programming language, libraries and data stores.\nThis model provides effective solutions for:\n\neffective and safe data storage,\ntransactional data recovery after a failure,\ndata access optimization,\nconcurrency management,\nevent processing -&gt; read -&gt; write\n\nAnd what if we are dealing with:\n\naggregation of data from many systems (e.g. for many stores),\nsupporting data analysis,\ndata reporting and summaries,\noptimization of complex queries,\nsupporting business decisions.\n\nResearch on such issues has led to the formulation of a new data processing model and a new type of database (Data warehouse).\nThis application design can cause problems when applications need to evolve or scale. Since multiple applications might work on the same data representation or share the same infrastructure, changing the schema of a table or scaling a database system requires careful planning and a lot of effort. Currently, many running applications (even in one area) are implemented as microservices, i.e. small and independent applications (LINUX programming philosophy - do little but right). Because microservices are strictly decoupled from each other and only communicate over well-defined interfaces, each microservice can be implemented with a different technology stack including a programming language, libraries and data stores.\nBoth are performed in batch mode. Today they are strictly made using Hadoop technology.",
    "crumbs": [
      "Home",
      "Lectures",
      "From files to Data Mash"
    ]
  },
  {
    "objectID": "lecture2.html",
    "href": "lecture2.html",
    "title": "Lecture 2 - Time to Stream",
    "section": "",
    "text": "Expectations vs Reality\n\n\nWhen to take a business decision?\n\n\n\n\n\nBatch = Big, historical datasets\nStream = stream data, online, generated and send continuously\n\n\n\n\n\nBatch = minutes, hours, days (Data warehouses)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = possible and used very often\nStream = ,,impossible’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2 - Time to Stream"
    ]
  },
  {
    "objectID": "lecture2.html#batch-vs-stream-processing",
    "href": "lecture2.html#batch-vs-stream-processing",
    "title": "Lecture 2 - Time to Stream",
    "section": "",
    "text": "Expectations vs Reality\n\n\nWhen to take a business decision?\n\n\n\n\n\nBatch = Big, historical datasets\nStream = stream data, online, generated and send continuously\n\n\n\n\n\nBatch = minutes, hours, days (Data warehouses)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = possible and used very often\nStream = ,,impossible’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2 - Time to Stream"
    ]
  },
  {
    "objectID": "lecture2.html#big-data",
    "href": "lecture2.html#big-data",
    "title": "Lecture 2 - Time to Stream",
    "section": "Big Data",
    "text": "Big Data\nBig Data system can be a part of (source of) data warehouses (ex. Data Lake, Enterprise Data Hub)\nBut Data Warehouses are not Big Data Systems!\n\nData warehouses\n\n\nhighly structured data retention\nfocused on the analysis and reporting process\n100 % accuracy\n\n\nBig Data\n\n\ndata of any structure\nserves a variety of data-driven purposes (analytics, data science …)\nless than 100 % accuracy\n\n\n\nELT process\nELT process is similar to ETL and has the same stages involved, but the order of performing ETL stages is different. Extract data from one or many sources and Load it to the destination system for example “data lake”. After that, You can Transform Your data a more dynamically on demand.\nUse Case: - Demanding scalability requirements of Big Data - Streaming analytics - Integration of highly distributed data sources - Multiple data products from the same sources\nELT is an emerging trend: - Big Data -&gt; Cloud computing - ELT separates the data pipeline from processing - More flexibility - No information loss (by transformations)\nDifference between ETL and ELT\n\nTransformations for ETL happen within the data pipeline\nTransformations for ELT happen in the destination environment\nETL is rigid - pipelines are engineered to user specifications\nELT is flexible - end users build their transformations\nOrganizations use ETL for relational data, on-premise - scalability is difficult\nELT solves scalability problems, handling both structured and unstructured Big Data in the cloud\nETL workflows take time to specify and develop\nELT supports self-serve, interactive analytics in real time\n\nELT is the evolution of ETL! - increasing demand for access to raw data.\nETL still has its place for many applications: - Lengthy time-to-insight - Challenges imposed by Big Data - Demand for access to siloed information\n\n\nData Extraction Techniques\nExamples of raw data sources: - Paper documents - Web pages - Analog audio/video - Survey, statistics, economics - Transactional data - Social media - Weather station networks - IoT - Medical records - Human genomes\nData extraction techniques include: - OCR - ADC sampling, CCD sampling - Mail, phone, or in-person surveys and polls - Cookies, user logs - Web scraping - API’s - Database querying - Edge computing - Biomedical devices\n\n\nData Transformation Techniques\ncan involve various operations, such as: - Data typing - Data structuring - Anonymizing, encrypting - Cleaning: duplicate records, missing values - Normalizing: converting data to common units - Filtering, sorting, aggregating, binning - Joining data sources\n\nSchema-on-write\nis the conventional ETL approach: - Consistency and efficiency - Limited versatility\n\n\nSchema-on-read\napplies to the modern ELT approach: - Versatility - Enhanced storage flexibility = more data\nTransformed data could be loss information. - data compression - filtering - aggregation - Edge computing devices\n\n\n\nData Load Techniques\n\nFull\nIncremental\nScheduled\nON-demand\nBatch and stream\npush and pull\nparallel and serial",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2 - Time to Stream"
    ]
  },
  {
    "objectID": "lecture2.html#hadoop-map-reduce",
    "href": "lecture2.html#hadoop-map-reduce",
    "title": "Lecture 2 - Time to Stream",
    "section": "Hadoop Map-Reduce",
    "text": "Hadoop Map-Reduce\n\n\nFind a simple map-reduce algorithm in any programming language and run it.\n\n\nHow to improve?\n\nAPACHE SPARK",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture 2 - Time to Stream"
    ]
  }
]